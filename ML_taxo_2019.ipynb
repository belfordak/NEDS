{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dinucleotide mutational fequencies in dsDNA viruses fed into unsupervised learning algortithms for clsutering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/belfordak/Desktop/ML_mutation_viral_taxonomy\n"
     ]
    }
   ],
   "source": [
    "cd /Users/belfordak/Desktop/ML_mutation_viral_taxonomy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Bio\n",
    "import Bio.Seq\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "def make_rc_record(record):\n",
    "    \"\"\"Returns a new SeqRecord with the reverse complement sequence.\"\"\"\n",
    "    return SeqRecord(seq = record.seq.reverse_complement(), \\\n",
    "                 id = \"rc_\" + record.id, \\\n",
    "                 description = \"reverse complement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    nucl  frequency        id #\n",
      "1     TA   0.747082  AB863202.1\n",
      "1     CC   1.091272  AB863202.1\n",
      "1     TC   1.365138  AB863202.1\n",
      "1     AG   1.204401  AB863202.1\n",
      "1     CA   1.000960  AB863202.1\n",
      "1     CT   1.205916  AB863202.1\n",
      "1     GA   1.188781  AB863202.1\n",
      "1     AC   0.757305  AB863202.1\n",
      "1     CG   0.658670  AB863202.1\n",
      "1     AA   1.058817  AB863202.1\n",
      "1     AT   0.950699  AB863202.1\n",
      "1     GT   0.734657  AB863202.1\n",
      "1     TG   0.946524  AB863202.1\n",
      "1     TT   1.119456  AB863202.1\n",
      "1     GG   1.036606  AB863202.1\n",
      "1     GC   0.927704  AB863202.1\n",
      "1    ATC   1.090485  AB863202.1\n",
      "1    TAT   1.149343  AB863202.1\n",
      "1    TCC   0.935720  AB863202.1\n",
      "1    CCA   1.082962  AB863202.1\n",
      "1    GTC   0.759233  AB863202.1\n",
      "1    GAC   1.001406  AB863202.1\n",
      "1    TGT   1.042701  AB863202.1\n",
      "1    TTG   1.020819  AB863202.1\n",
      "1    GCC   1.097401  AB863202.1\n",
      "1    GAG   0.744399  AB863202.1\n",
      "1    TCG   0.995386  AB863202.1\n",
      "1    CGA   0.979199  AB863202.1\n",
      "1    TTC   1.077251  AB863202.1\n",
      "1    CTA   1.034123  AB863202.1\n",
      "..   ...        ...         ...\n",
      "765  GGA   1.156975  MH427217.1\n",
      "765  ACT   0.845142  MH427217.1\n",
      "765  CAA   1.044405  MH427217.1\n",
      "765  AAC   1.099400  MH427217.1\n",
      "765  ATT   0.895397  MH427217.1\n",
      "765  GTA   1.094310  MH427217.1\n",
      "765  GGG   0.847710  MH427217.1\n",
      "765  ACC   1.004301  MH427217.1\n",
      "765  CGG   0.865332  MH427217.1\n",
      "765  AGT   0.848088  MH427217.1\n",
      "765  AGA   1.132150  MH427217.1\n",
      "765  CCG   0.849845  MH427217.1\n",
      "765  AGG   1.010307  MH427217.1\n",
      "765  GCG   1.059649  MH427217.1\n",
      "765  TGT   1.055150  MH427217.1\n",
      "765  TTG   1.045125  MH427217.1\n",
      "765  GCC   0.825210  MH427217.1\n",
      "765  TCG   0.889101  MH427217.1\n",
      "765  GAG   0.899193  MH427217.1\n",
      "765  ATC   1.112811  MH427217.1\n",
      "765  TAT   1.043737  MH427217.1\n",
      "765  TCC   1.150925  MH427217.1\n",
      "765  CCA   1.131418  MH427217.1\n",
      "765  GAC   0.763054  MH427217.1\n",
      "765  GTC   0.755900  MH427217.1\n",
      "765  CTA   1.011269  MH427217.1\n",
      "765  TAC   1.099967  MH427217.1\n",
      "765  GTG   0.893301  MH427217.1\n",
      "765  CGA   0.870510  MH427217.1\n",
      "765  TTC   1.017746  MH427217.1\n",
      "\n",
      "[61234 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "all_viruses = pd.read_csv(\"./input_data/all_viruses.fasta.full.txt\", header = None, sep='\\t', names = [\"id #\", \"dinucl\", \"frequency\", \"actual id\"])\n",
    "\n",
    "all_viruses.columns = [\"nucl\", \"frequency\", \"id #\", \"actual id\"]\n",
    "no_RC = pd.DataFrame(all_viruses.iloc[:,0:3])\n",
    "#no_RC.index = no_RC['nucl']\n",
    "print(no_RC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id #</th>\n",
       "      <th>AB027020.1</th>\n",
       "      <th>AB027021.1</th>\n",
       "      <th>AB040456.1</th>\n",
       "      <th>AB048545.1</th>\n",
       "      <th>AB048546.1</th>\n",
       "      <th>AB048547.1</th>\n",
       "      <th>AB048548.1</th>\n",
       "      <th>AB048549.1</th>\n",
       "      <th>AB048550.1</th>\n",
       "      <th>AB048551.1</th>\n",
       "      <th>...</th>\n",
       "      <th>U31790.1</th>\n",
       "      <th>U31791.1</th>\n",
       "      <th>U31792.1</th>\n",
       "      <th>U31793.1</th>\n",
       "      <th>U31794.1</th>\n",
       "      <th>U61771.1</th>\n",
       "      <th>U85660.1</th>\n",
       "      <th>X70827.1</th>\n",
       "      <th>X70828.1</th>\n",
       "      <th>X70829.1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nucl</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AA</th>\n",
       "      <td>0.978133</td>\n",
       "      <td>0.990595</td>\n",
       "      <td>0.959662</td>\n",
       "      <td>1.191219</td>\n",
       "      <td>1.190260</td>\n",
       "      <td>1.196500</td>\n",
       "      <td>1.203301</td>\n",
       "      <td>1.203771</td>\n",
       "      <td>1.198643</td>\n",
       "      <td>1.195276</td>\n",
       "      <td>...</td>\n",
       "      <td>1.004574</td>\n",
       "      <td>0.960219</td>\n",
       "      <td>1.003705</td>\n",
       "      <td>0.971734</td>\n",
       "      <td>0.959888</td>\n",
       "      <td>1.200410</td>\n",
       "      <td>0.992599</td>\n",
       "      <td>0.995676</td>\n",
       "      <td>0.989518</td>\n",
       "      <td>1.010921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAA</th>\n",
       "      <td>1.110681</td>\n",
       "      <td>1.274264</td>\n",
       "      <td>1.281866</td>\n",
       "      <td>1.192506</td>\n",
       "      <td>1.191252</td>\n",
       "      <td>1.171192</td>\n",
       "      <td>1.139543</td>\n",
       "      <td>1.134316</td>\n",
       "      <td>1.130467</td>\n",
       "      <td>1.175445</td>\n",
       "      <td>...</td>\n",
       "      <td>1.140071</td>\n",
       "      <td>1.157149</td>\n",
       "      <td>1.064476</td>\n",
       "      <td>1.261283</td>\n",
       "      <td>1.184845</td>\n",
       "      <td>1.174010</td>\n",
       "      <td>1.098694</td>\n",
       "      <td>1.079097</td>\n",
       "      <td>1.120421</td>\n",
       "      <td>1.093777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAC</th>\n",
       "      <td>0.979673</td>\n",
       "      <td>0.850991</td>\n",
       "      <td>0.916453</td>\n",
       "      <td>0.915750</td>\n",
       "      <td>0.936256</td>\n",
       "      <td>0.946565</td>\n",
       "      <td>0.955732</td>\n",
       "      <td>0.981537</td>\n",
       "      <td>0.956892</td>\n",
       "      <td>0.941186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.861752</td>\n",
       "      <td>1.005146</td>\n",
       "      <td>0.996788</td>\n",
       "      <td>0.855803</td>\n",
       "      <td>1.015595</td>\n",
       "      <td>0.926441</td>\n",
       "      <td>0.970794</td>\n",
       "      <td>0.974835</td>\n",
       "      <td>0.975599</td>\n",
       "      <td>1.085286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAG</th>\n",
       "      <td>0.912075</td>\n",
       "      <td>0.904625</td>\n",
       "      <td>0.914623</td>\n",
       "      <td>0.886098</td>\n",
       "      <td>0.881644</td>\n",
       "      <td>0.892424</td>\n",
       "      <td>0.914371</td>\n",
       "      <td>0.911880</td>\n",
       "      <td>0.926738</td>\n",
       "      <td>0.883504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899192</td>\n",
       "      <td>0.873560</td>\n",
       "      <td>0.893496</td>\n",
       "      <td>1.057520</td>\n",
       "      <td>0.883551</td>\n",
       "      <td>0.889744</td>\n",
       "      <td>0.940643</td>\n",
       "      <td>0.868866</td>\n",
       "      <td>0.919637</td>\n",
       "      <td>0.809516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAT</th>\n",
       "      <td>0.950863</td>\n",
       "      <td>0.896359</td>\n",
       "      <td>0.866944</td>\n",
       "      <td>0.889573</td>\n",
       "      <td>0.880084</td>\n",
       "      <td>0.889538</td>\n",
       "      <td>0.908211</td>\n",
       "      <td>0.900244</td>\n",
       "      <td>0.910122</td>\n",
       "      <td>0.900218</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000290</td>\n",
       "      <td>0.915460</td>\n",
       "      <td>1.006503</td>\n",
       "      <td>0.838770</td>\n",
       "      <td>0.876521</td>\n",
       "      <td>0.904305</td>\n",
       "      <td>0.962468</td>\n",
       "      <td>1.035466</td>\n",
       "      <td>0.953714</td>\n",
       "      <td>1.014333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 765 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "id #  AB027020.1  AB027021.1  AB040456.1  AB048545.1  AB048546.1  AB048547.1  \\\n",
       "nucl                                                                           \n",
       "AA      0.978133    0.990595    0.959662    1.191219    1.190260    1.196500   \n",
       "AAA     1.110681    1.274264    1.281866    1.192506    1.191252    1.171192   \n",
       "AAC     0.979673    0.850991    0.916453    0.915750    0.936256    0.946565   \n",
       "AAG     0.912075    0.904625    0.914623    0.886098    0.881644    0.892424   \n",
       "AAT     0.950863    0.896359    0.866944    0.889573    0.880084    0.889538   \n",
       "\n",
       "id #  AB048548.1  AB048549.1  AB048550.1  AB048551.1    ...     U31790.1  \\\n",
       "nucl                                                    ...                \n",
       "AA      1.203301    1.203771    1.198643    1.195276    ...     1.004574   \n",
       "AAA     1.139543    1.134316    1.130467    1.175445    ...     1.140071   \n",
       "AAC     0.955732    0.981537    0.956892    0.941186    ...     0.861752   \n",
       "AAG     0.914371    0.911880    0.926738    0.883504    ...     0.899192   \n",
       "AAT     0.908211    0.900244    0.910122    0.900218    ...     1.000290   \n",
       "\n",
       "id #  U31791.1  U31792.1  U31793.1  U31794.1  U61771.1  U85660.1  X70827.1  \\\n",
       "nucl                                                                         \n",
       "AA    0.960219  1.003705  0.971734  0.959888  1.200410  0.992599  0.995676   \n",
       "AAA   1.157149  1.064476  1.261283  1.184845  1.174010  1.098694  1.079097   \n",
       "AAC   1.005146  0.996788  0.855803  1.015595  0.926441  0.970794  0.974835   \n",
       "AAG   0.873560  0.893496  1.057520  0.883551  0.889744  0.940643  0.868866   \n",
       "AAT   0.915460  1.006503  0.838770  0.876521  0.904305  0.962468  1.035466   \n",
       "\n",
       "id #  X70828.1  X70829.1  \n",
       "nucl                      \n",
       "AA    0.989518  1.010921  \n",
       "AAA   1.120421  1.093777  \n",
       "AAC   0.975599  1.085286  \n",
       "AAG   0.919637  0.809516  \n",
       "AAT   0.953714  1.014333  \n",
       "\n",
       "[5 rows x 765 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#pivoting, like melting, data for column selction to new df \n",
    "df = no_RC.pivot(index='id #', columns='nucl', values='frequency') \n",
    "df2 = no_RC.pivot(index='nucl', columns='id #', values='frequency')\n",
    "df22 = df2.iloc[1:] \n",
    "df22.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>nucl</th>\n",
       "      <th>AA</th>\n",
       "      <th>AAA</th>\n",
       "      <th>AAC</th>\n",
       "      <th>AAG</th>\n",
       "      <th>AAT</th>\n",
       "      <th>AC</th>\n",
       "      <th>ACA</th>\n",
       "      <th>ACC</th>\n",
       "      <th>ACG</th>\n",
       "      <th>ACT</th>\n",
       "      <th>...</th>\n",
       "      <th>TTA</th>\n",
       "      <th>TTC</th>\n",
       "      <th>TTG</th>\n",
       "      <th>TTN</th>\n",
       "      <th>TTT</th>\n",
       "      <th>TTY</th>\n",
       "      <th>TY</th>\n",
       "      <th>TYA</th>\n",
       "      <th>YA</th>\n",
       "      <th>YAT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id #</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AB027020.1</th>\n",
       "      <td>0.978133</td>\n",
       "      <td>1.110681</td>\n",
       "      <td>0.979673</td>\n",
       "      <td>0.912075</td>\n",
       "      <td>0.950863</td>\n",
       "      <td>1.211614</td>\n",
       "      <td>1.117282</td>\n",
       "      <td>0.960852</td>\n",
       "      <td>1.102153</td>\n",
       "      <td>0.812945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.902286</td>\n",
       "      <td>1.041294</td>\n",
       "      <td>0.838897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.227367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AB027021.1</th>\n",
       "      <td>0.990595</td>\n",
       "      <td>1.274264</td>\n",
       "      <td>0.850991</td>\n",
       "      <td>0.904625</td>\n",
       "      <td>0.896359</td>\n",
       "      <td>1.208663</td>\n",
       "      <td>1.095106</td>\n",
       "      <td>0.991547</td>\n",
       "      <td>1.002876</td>\n",
       "      <td>0.853087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941865</td>\n",
       "      <td>1.013043</td>\n",
       "      <td>0.848438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.187126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AB040456.1</th>\n",
       "      <td>0.959662</td>\n",
       "      <td>1.281866</td>\n",
       "      <td>0.916453</td>\n",
       "      <td>0.914623</td>\n",
       "      <td>0.866944</td>\n",
       "      <td>1.171204</td>\n",
       "      <td>1.120733</td>\n",
       "      <td>0.878469</td>\n",
       "      <td>1.160130</td>\n",
       "      <td>0.875251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938814</td>\n",
       "      <td>1.102136</td>\n",
       "      <td>0.813244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.226717</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AB048545.1</th>\n",
       "      <td>1.191219</td>\n",
       "      <td>1.192506</td>\n",
       "      <td>0.915750</td>\n",
       "      <td>0.886098</td>\n",
       "      <td>0.889573</td>\n",
       "      <td>0.844430</td>\n",
       "      <td>1.080808</td>\n",
       "      <td>0.945736</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.938548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.887327</td>\n",
       "      <td>1.119273</td>\n",
       "      <td>0.756900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.164461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AB048546.1</th>\n",
       "      <td>1.190260</td>\n",
       "      <td>1.191252</td>\n",
       "      <td>0.936256</td>\n",
       "      <td>0.881644</td>\n",
       "      <td>0.880084</td>\n",
       "      <td>0.850447</td>\n",
       "      <td>1.082225</td>\n",
       "      <td>0.963618</td>\n",
       "      <td>1.237981</td>\n",
       "      <td>0.923617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.892376</td>\n",
       "      <td>1.120564</td>\n",
       "      <td>0.762049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.159568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "nucl              AA       AAA       AAC       AAG       AAT        AC  \\\n",
       "id #                                                                     \n",
       "AB027020.1  0.978133  1.110681  0.979673  0.912075  0.950863  1.211614   \n",
       "AB027021.1  0.990595  1.274264  0.850991  0.904625  0.896359  1.208663   \n",
       "AB040456.1  0.959662  1.281866  0.916453  0.914623  0.866944  1.171204   \n",
       "AB048545.1  1.191219  1.192506  0.915750  0.886098  0.889573  0.844430   \n",
       "AB048546.1  1.190260  1.191252  0.936256  0.881644  0.880084  0.850447   \n",
       "\n",
       "nucl             ACA       ACC       ACG       ACT ...        TTA       TTC  \\\n",
       "id #                                               ...                        \n",
       "AB027020.1  1.117282  0.960852  1.102153  0.812945 ...   0.902286  1.041294   \n",
       "AB027021.1  1.095106  0.991547  1.002876  0.853087 ...   0.941865  1.013043   \n",
       "AB040456.1  1.120733  0.878469  1.160130  0.875251 ...   0.938814  1.102136   \n",
       "AB048545.1  1.080808  0.945736  1.250000  0.938548 ...   0.887327  1.119273   \n",
       "AB048546.1  1.082225  0.963618  1.237981  0.923617 ...   0.892376  1.120564   \n",
       "\n",
       "nucl             TTG  TTN       TTT  TTY  TY  TYA  YA  YAT  \n",
       "id #                                                        \n",
       "AB027020.1  0.838897  NaN  1.227367  NaN NaN  NaN NaN  NaN  \n",
       "AB027021.1  0.848438  NaN  1.187126  NaN NaN  NaN NaN  NaN  \n",
       "AB040456.1  0.813244  NaN  1.226717  NaN NaN  NaN NaN  NaN  \n",
       "AB048545.1  0.756900  NaN  1.164461  NaN NaN  NaN NaN  NaN  \n",
       "AB048546.1  0.762049  NaN  1.159568  NaN NaN  NaN NaN  NaN  \n",
       "\n",
       "[5 rows x 116 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest_T= df22.T\n",
    "dftest_T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now sepreating out di and tri nucls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AA', 'AAA', 'AAC', 'AAG', 'AAT', 'AC', 'ACA', 'ACC', 'ACG', 'ACT',\n",
       "       ...\n",
       "       'TTA', 'TTC', 'TTG', 'TTN', 'TTT', 'TTY', 'TY', 'TYA', 'YA', 'YAT'],\n",
       "      dtype='object', name='nucl', length=116)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dftest_T.index\n",
    "dftest_T.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AN',\n",
       " 'ANA',\n",
       " 'ANG',\n",
       " 'GAN',\n",
       " 'GTN',\n",
       " 'NAG',\n",
       " 'NC',\n",
       " 'NCT',\n",
       " 'NG',\n",
       " 'NGG',\n",
       " 'NT',\n",
       " 'NTA',\n",
       " 'TAN',\n",
       " 'TN',\n",
       " 'TNC',\n",
       " 'TNT',\n",
       " 'TTN']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dftest_T.filter(regex='N'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retranspose again b/c first time retain original indecies, so that's why it's weird above. anyway..\n",
    "#two_df = dftest[two.values].T\n",
    "three_df = dftest[three.values].T\n",
    "print(three_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(765, 99)\n",
      "nucl              AA       AAA       AAC       AAG       AAT        AC  \\\n",
      "id #                                                                     \n",
      "AB027020.1  0.978133  1.110681  0.979673  0.912075  0.950863  1.211614   \n",
      "AB027021.1  0.990595  1.274264  0.850991  0.904625  0.896359  1.208663   \n",
      "AB040456.1  0.959662  1.281866  0.916453  0.914623  0.866944  1.171204   \n",
      "AB048545.1  1.191219  1.192506  0.915750  0.886098  0.889573  0.844430   \n",
      "AB048546.1  1.190260  1.191252  0.936256  0.881644  0.880084  0.850447   \n",
      "\n",
      "nucl             ACA       ACC       ACG       ACT    ...           TG  \\\n",
      "id #                                                  ...                \n",
      "AB027020.1  1.117282  0.960852  1.102153  0.812945    ...     1.326947   \n",
      "AB027021.1  1.095106  0.991547  1.002876  0.853087    ...     1.362019   \n",
      "AB040456.1  1.120733  0.878469  1.160130  0.875251    ...     1.336960   \n",
      "AB048545.1  1.080808  0.945736  1.250000  0.938548    ...     1.179611   \n",
      "AB048546.1  1.082225  0.963618  1.237981  0.923617    ...     1.193012   \n",
      "\n",
      "nucl             TGA       TGC       TGG       TGT        TT       TTA  \\\n",
      "id #                                                                     \n",
      "AB027020.1  0.685103  1.034243  1.106594  1.129946  1.036852  0.902286   \n",
      "AB027021.1  0.716600  1.005808  0.953589  1.228367  1.073013  0.941865   \n",
      "AB040456.1  0.684964  0.922799  0.966423  1.304576  1.044271  0.938814   \n",
      "AB048545.1  0.954851  1.093722  0.950114  1.015435  1.242326  0.887327   \n",
      "AB048546.1  0.960642  1.117838  0.964682  0.968644  1.241562  0.892376   \n",
      "\n",
      "nucl             TTC       TTG       TTT  \n",
      "id #                                      \n",
      "AB027020.1  1.041294  0.838897  1.227367  \n",
      "AB027021.1  1.013043  0.848438  1.187126  \n",
      "AB040456.1  1.102136  0.813244  1.226717  \n",
      "AB048545.1  1.119273  0.756900  1.164461  \n",
      "AB048546.1  1.120564  0.762049  1.159568  \n",
      "\n",
      "[5 rows x 80 columns]\n"
     ]
    }
   ],
   "source": [
    "#Now filter out any \"N\"ds or \"S\"s\n",
    "#for dinucl\n",
    "temp = dftest_T[dftest_T.columns.drop(list(dftest_T.filter(regex='N')))]\n",
    "print(temp.shape)\n",
    "\n",
    "temp2 = temp[temp.columns.drop(list(temp.filter(regex='S')))]\n",
    "\n",
    "temp3 = temp2[temp2.columns.drop(list(temp2.filter(regex='n')))]\n",
    "\n",
    "temp4 = temp3[temp3.columns.drop(list(temp3.filter(regex='Y')))]\n",
    "\n",
    "temp5 = temp4[temp4.columns.drop(list(temp4.filter(regex='R')))]\n",
    "\n",
    "#print(temp5.columns)\n",
    "print(temp5.head())\n",
    "yy = temp5\n",
    "\n",
    "yy.to_csv(\"./25Feb_noRevComp/di_and_tri_nucls_FEB_man_split.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two = pd.Series(yy.columns.str.len() == 2)\n",
    "three = pd.Series(yy.columns.str.len() == 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yy[three.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now filter out any \"N\"ds or \"S\"s\n",
    "#for dinucl\n",
    "temp2_1 = two_df[two_df.columns.drop(list(two_df.filter(regex='N')))]\n",
    "#print(temp2_1.columns)\n",
    "temp2_2 = temp2_1[temp2_1.columns.drop(list(temp2_1.filter(regex='S')))]\n",
    "print(temp2_2.columns)\n",
    "\n",
    "two_df = temp2_2\n",
    "\n",
    "#for trinucl\n",
    "temp3_1 = three_df[three_df.columns.drop(list(three_df.filter(regex='N')))]\n",
    "print(temp3_1.columns)\n",
    "temp3_2 = temp3_1[temp3_1.columns.drop(list(temp3_1.filter(regex='S')))]\n",
    "#print(temp3_2.columns)\n",
    "\n",
    "three_df = temp3_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testcol = three_df['AAA'].div(three_df.groupby.cols['AAA'].shift(4))\n",
    "#print(testcol.head())\n",
    "#print(three_df['AAA'].shift(4))\n",
    "print(three_df)\n",
    "three_df.to_csv(\"/Users/belfordak/Desktop/ML_mutation_viral_taxonomy/three_df_Feb2019.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#three_df.iloc[:, ::4]\n",
    "#print(three_df.iloc[:,0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Now running the calculation on the second position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 arry trint\n",
    "#2 array sub\n",
    "#loops\n",
    "#naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trint = (list(itertools.product('ATCG','ATCG')))\n",
    "trint_asarry = np.asarray(trint)\n",
    "print(trint_asarry)\n",
    "\n",
    "sub = (['C', 'A'],['C', 'T'],['C', 'G'],['T', 'A'],['T', 'C'],['T', 'G'])\n",
    "sub_asarry = np.asarray(sub)\n",
    "print(sub_asarry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naming = str([trint_asarry[0,0] + \"[\" + sub_asarry[0,0] + \">\" + sub_asarry[0,1] +\"]\" + trint_asarry[0,1]])\n",
    "print(naming)\n",
    "#sub_asarry\n",
    "#print(naming)\n",
    "print(str([trint_asarry[1,0] + \"[\" + sub_asarry[1,0] + \">\" + sub_asarry[1,1] +\"]\" + trint_asarry[1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in three_df.iterrows():\n",
    "    print(row['AAA']) #/ row['ACA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(three_df.iloc[0,:])\n",
    "print(three_df.index)\n",
    "print(len(three_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "while i <= len(three_df.index):\n",
    "    print(col_calc, index = col_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.empty((0,len(three_df.index)), int)\n",
    "arr\n",
    "\n",
    "#OR\n",
    "arr = np.empty((0,97), int)\n",
    "arr\n",
    "\n",
    "#OR\n",
    "arr = np.empty((len(three_df.index),0), int)\n",
    "arr\n",
    "\n",
    "#OR\n",
    "\n",
    "arr = np.empty(((len(three_df.index)),97), int)\n",
    "arr.shape\n",
    "\n",
    "#i dont think this helps me since i have to loop \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "#just make sure it's a df\n",
    "for index, row in three_df.iterrows(): #for i at 0 until i is greater than # rows OR new var = array of new row. append (empty array to populate) \n",
    "   #append new row to new df\n",
    "    for t in trint_asarry: \n",
    "        for i in sub_asarry:\n",
    "            ref = str(t[0] + i[0] + t[1]) #x3 b/c 3 possible flanking\n",
    "            varr = str(t[0] + i[1] + t[1]) #rotates through base subs\n",
    "            col_name = (t[0] + \"[\" + i[0] + \">\" + i[1] + \"]\" + t[1])\n",
    "            col_calc = ((row[varr] / row[ref])) #call i and col, name = col_name\n",
    "            #data.concat((col_calc))\n",
    "            data.append(col_calc) # index = col_name\n",
    "            #print(varr)\n",
    "        datap = np.asarray(data)\n",
    "        print(datap)\n",
    "                   \n",
    "            \n",
    "            #print(row[ref] / row[varr])\n",
    "            \n",
    " #my_columns = [\"a\", \"b\", \"c\"]\n",
    "#>>> data.columns = col_name\n",
    "#>>> print df           \n",
    "            #print(i[0],i[1])\n",
    "            #print(str(t[0] + \"[\" + i[0] + \">\" + i[1] + \"]\" + t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datap = [] #just make sure it's a df\n",
    "for i, row in three_df.iterrows(): #for i at 0 until i is greater than # rows OR new var = array of new row. append (empty array to populate) \n",
    "   #append new row to new df\n",
    "    for t in trint_asarry: \n",
    "        for i in sub_asarry:\n",
    "            ref = str(t[0] + i[0] + t[1]) #x3 b/c 3 possible flanking\n",
    "            varr = str(t[0] + i[1] + t[1]) #rotates through base subs\n",
    "            col_calc = pd.Series(row[varr] / row[ref]) #call i and col\n",
    "            col_name = list(t[0] + \"[\" + i[0] + \">\" + i[1] + \"]\" + t[1])\n",
    "            #data = pd.concat([data,pd.DataFrame(columns = col_name)])\n",
    "            df = pd.DataFrame(col_calc.head())\n",
    "            #df.columns = col_name\n",
    "            print((col_calc))\n",
    "            \n",
    "                   \n",
    "            \n",
    "            #print(row[ref] / row[varr])\n",
    "            \n",
    " #my_columns = [\"a\", \"b\", \"c\"]\n",
    "#>>> data.columns = col_name\n",
    "#>>> print df           \n",
    "            #print(i[0],i[1])\n",
    "            #print(str(t[0] + \"[\" + i[0] + \">\" + i[1] + \"]\" + t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.new_df_test = []\n",
    "while i <= three_df.shape[0]:  \n",
    "    for t in trint_asarry: \n",
    "        for i in sub_asarry:\n",
    "            ref = str(t[0] + i[0] + t[1]) #x3 b/c 3 possible flanking\n",
    "            varr = str(t[0] + i[1] + t[1]) #rotates through base subs\n",
    "            col = (row[varr] / row[ref]) # call i and col\n",
    "            col_name = str(t[0] + \"[\" + i[0] + \">\" + i[1] + \"]\" + t[1])\n",
    "            pd.new_df_test.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(three_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_df = pd.read_csv(\"./25Feb_noRevComp/just_trinucls.csv\")\n",
    "three_df = three_df.set_index('id #')\n",
    "three_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations_df = pd.DataFrame()\n",
    "mutations_df['A[C>A]A'] = three_df['AAA']/three_df['ACA']\n",
    "mutations_df['A[C>A]C'] = three_df['AAC']/three_df['ACC']\n",
    "mutations_df['A[C>A]G'] = three_df['AAG']/three_df['ACG']\n",
    "mutations_df['A[C>A]T'] = three_df['AAT']/three_df['ACT']\n",
    "mutations_df['A[C>G]A'] = three_df['AGA']/three_df['ACA']\n",
    "mutations_df['A[C>G]C'] = three_df['AGC']/three_df['ACC']\n",
    "mutations_df['A[C>G]G'] = three_df['AGG']/three_df['ACG']\n",
    "mutations_df['A[C>G]T'] = three_df['AGT']/three_df['ACT'] ##\n",
    "mutations_df['A[C>T]A'] = three_df['ATA']/three_df['ACA']\n",
    "mutations_df['A[C>T]C'] = three_df['ATC']/three_df['ACC']\n",
    "mutations_df['A[C>T]G'] = three_df['ATG']/three_df['ACG']\n",
    "mutations_df['A[C>T]T'] = three_df['ATT']/three_df['ACT']\n",
    "mutations_df['A[T>A]A'] = three_df['AAA']/three_df['ATA']\n",
    "mutations_df['A[T>A]C'] = three_df['AAC']/three_df['ATC']\n",
    "mutations_df['A[T>A]G'] = three_df['AAG']/three_df['ATG']\n",
    "mutations_df['A[T>A]T'] = three_df['AAT']/three_df['ATT'] ##\n",
    "mutations_df['A[T>C]A'] = three_df['ACA']/three_df['ATA']\n",
    "mutations_df['A[T>C]C'] = three_df['ACC']/three_df['ATC']\n",
    "mutations_df['A[T>C]G'] = three_df['ACG']/three_df['ATG']\n",
    "mutations_df['A[T>C]T'] = three_df['ACT']/three_df['ATT']\n",
    "mutations_df['A[T>G]A'] = three_df['AGA']/three_df['ATA']\n",
    "mutations_df['A[T>G]C'] = three_df['AGC']/three_df['ATC']\n",
    "mutations_df['A[T>G]G'] = three_df['AGG']/three_df['ATG']\n",
    "mutations_df['A[T>G]T'] = three_df['AGT']/three_df['ATT']\n",
    "mutations_df['C[C>A]A'] = three_df['CAA']/three_df['CCA']\n",
    "mutations_df['C[C>A]C'] = three_df['CAC']/three_df['CCC']\n",
    "mutations_df['C[C>A]G'] = three_df['CAG']/three_df['CCG']\n",
    "mutations_df['C[C>A]T'] = three_df['CAT']/three_df['CCT']\n",
    "mutations_df['C[C>G]A'] = three_df['CGA']/three_df['CCA']\n",
    "mutations_df['C[C>G]C'] = three_df['CGC']/three_df['CCC']\n",
    "mutations_df['C[C>G]G'] = three_df['CGG']/three_df['CCG'] ##\n",
    "mutations_df['C[C>G]T'] = three_df['CGT']/three_df['CCT']\n",
    "mutations_df['C[C>T]A'] = three_df['CTA']/three_df['CCA']\n",
    "mutations_df['C[C>T]C'] = three_df['CTC']/three_df['CCC']\n",
    "mutations_df['C[C>T]G'] = three_df['CTG']/three_df['CCG']\n",
    "mutations_df['C[C>T]T'] = three_df['CTT']/three_df['CCT']\n",
    "mutations_df['C[T>A]A'] = three_df['CAA']/three_df['CTA']\n",
    "mutations_df['C[T>A]C'] = three_df['CAC']/three_df['CTC']\n",
    "mutations_df['C[T>A]G'] = three_df['CAG']/three_df['CTG'] ##\n",
    "mutations_df['C[T>A]T'] = three_df['CAT']/three_df['CTT']\n",
    "mutations_df['C[T>C]A'] = three_df['CCA']/three_df['CTA']\n",
    "mutations_df['C[T>C]C'] = three_df['CCC']/three_df['CTC']\n",
    "mutations_df['C[T>C]G'] = three_df['CCG']/three_df['CTG']\n",
    "mutations_df['C[T>C]T'] = three_df['CCT']/three_df['CTT']\n",
    "mutations_df['C[T>G]A'] = three_df['CGA']/three_df['CTA']\n",
    "mutations_df['C[T>G]C'] = three_df['CGC']/three_df['CTC']\n",
    "mutations_df['C[T>G]G'] = three_df['CGG']/three_df['CTG']\n",
    "mutations_df['C[T>G]T'] = three_df['CGT']/three_df['CTT']\n",
    "mutations_df['G[C>A]A'] = three_df['GAA']/three_df['GCA']\n",
    "mutations_df['G[C>A]C'] = three_df['GAC']/three_df['GCC']\n",
    "mutations_df['G[C>A]G'] = three_df['GAG']/three_df['GCG']\n",
    "mutations_df['G[C>A]T'] = three_df['GAT']/three_df['GCT']\n",
    "mutations_df['G[C>G]A'] = three_df['GGA']/three_df['GCA']\n",
    "mutations_df['G[C>G]C'] = three_df['GGC']/three_df['GCC']##\n",
    "mutations_df['G[C>G]G'] = three_df['GGG']/three_df['GCG'] \n",
    "mutations_df['G[C>G]T'] = three_df['GGT']/three_df['GCT']\n",
    "mutations_df['G[C>T]A'] = three_df['GTA']/three_df['GCA']\n",
    "mutations_df['G[C>T]C'] = three_df['GTC']/three_df['GCC']\n",
    "mutations_df['G[C>T]G'] = three_df['GTG']/three_df['GCG']\n",
    "mutations_df['G[C>T]T'] = three_df['GTT']/three_df['GCT']\n",
    "mutations_df['G[T>A]A'] = three_df['GAA']/three_df['GTA']\n",
    "mutations_df['G[T>A]C'] = three_df['GAC']/three_df['GTC'] ##\n",
    "mutations_df['G[T>A]G'] = three_df['GAG']/three_df['GTG']\n",
    "mutations_df['G[T>A]T'] = three_df['GAT']/three_df['GTT']\n",
    "mutations_df['G[T>C]A'] = three_df['GCA']/three_df['GTA']\n",
    "mutations_df['G[T>C]C'] = three_df['GCC']/three_df['GTC']\n",
    "mutations_df['G[T>C]G'] = three_df['GCG']/three_df['GTG']\n",
    "mutations_df['G[T>C]T'] = three_df['GCT']/three_df['GTT']\n",
    "mutations_df['G[T>G]A'] = three_df['GGA']/three_df['GTA']\n",
    "mutations_df['G[T>G]C'] = three_df['GGC']/three_df['GTC']\n",
    "mutations_df['G[T>G]G'] = three_df['GGG']/three_df['GTG']\n",
    "mutations_df['G[T>G]T'] = three_df['GGT']/three_df['GTT']\n",
    "mutations_df['T[C>A]A'] = three_df['TAA']/three_df['TCA']\n",
    "mutations_df['T[C>A]C'] = three_df['TAC']/three_df['TCC']\n",
    "mutations_df['T[C>A]G'] = three_df['TAG']/three_df['TCG']\n",
    "mutations_df['T[C>A]T'] = three_df['TAT']/three_df['TCT']\n",
    "mutations_df['T[C>G]A'] = three_df['TGA']/three_df['TCA'] ##\n",
    "mutations_df['T[C>G]C'] = three_df['TGC']/three_df['TCC']\n",
    "mutations_df['T[C>G]G'] = three_df['TGG']/three_df['TCG']\n",
    "mutations_df['T[C>G]T'] = three_df['TGT']/three_df['TCT']\n",
    "mutations_df['T[C>T]A'] = three_df['TTA']/three_df['TCA']\n",
    "mutations_df['T[C>T]C'] = three_df['TTC']/three_df['TCC']\n",
    "mutations_df['T[C>T]G'] = three_df['TTG']/three_df['TCG']\n",
    "mutations_df['T[C>T]T'] = three_df['TTT']/three_df['TCT']\n",
    "mutations_df['T[T>A]A'] = three_df['TAA']/three_df['TTA'] ##\n",
    "mutations_df['T[T>A]C'] = three_df['TAC']/three_df['TTC']\n",
    "mutations_df['T[T>A]G'] = three_df['TAG']/three_df['TTG']\n",
    "mutations_df['T[T>A]T'] = three_df['TAT']/three_df['TTT']\n",
    "mutations_df['T[T>C]A'] = three_df['TCA']/three_df['TTA']\n",
    "mutations_df['T[T>C]C'] = three_df['TCC']/three_df['TTC']\n",
    "mutations_df['T[T>C]G'] = three_df['TCG']/three_df['TTG']\n",
    "mutations_df['T[T>C]T'] = three_df['TCT']/three_df['TTT']\n",
    "mutations_df['T[T>G]A'] = three_df['TGA']/three_df['TTA']\n",
    "mutations_df['T[T>G]C'] = three_df['TGC']/three_df['TTC']\n",
    "mutations_df['T[T>G]G'] = three_df['TGG']/three_df['TTG']\n",
    "mutations_df['T[T>G]T'] = three_df['TGT']/three_df['TTT']\n",
    "\n",
    "#print(mutations_df) #why 96, but not my problem rn, this list was taken from the cancer atlas' \"signatures_probabilites.txt table that I'll use later on)\"\n",
    "#print(mutations_df['A[C>G]T'])\n",
    "\n",
    "print(three_df['AGT'])\n",
    "#print(three_df['ACT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations_df.to_csv(\"/Users/belfordak/Desktop/ML_mutation_viral_taxonomy/25Feb_noRevComp/new_w_subs_Feb2019.csv\")\n",
    "#/Users/belfordak/Desktop/ML_mutation_viral_taxonomy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ML model starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans #already loaded\n",
    "from sklearn.datasets import make_regression\n",
    "import seaborn as sns\n",
    "import scipy.spatial.distance\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#may or may not use all of these\n",
    "import sklearn.metrics\n",
    "import sklearn.datasets\n",
    "import sklearn.manifold\n",
    "import sklearn.decomposition\n",
    "import sklearn.preprocessing\n",
    "import sklearn.cluster\n",
    "import sklearn.feature_selection\n",
    "import sklearn.ensemble\n",
    "import sklearn.svm\n",
    "import sklearn.model_selection\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace NaNs with 0s...later look into why theyre there\n",
    "\n",
    "mutations_df_noNA = mutations_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.heatmap(mutations_df) #don't think this is actually valuable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(mutations_df_noNA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction tests. It's def necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Dimensionality\", mutations_df_noNA.shape)\n",
    "D = sklearn.metrics.pairwise_distances(mutations_df_noNA, metric='braycurtis', n_jobs=-1)\n",
    "print(\"Pairwise distances:\")\n",
    "sns.heatmap(D, robust=True, square=True, yticklabels=False, xticklabels=False, cbar=True)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(np.hstack(D), 20, facecolor='orange', alpha=0.5)\n",
    "plt.xlabel('Distribution of pairwise distances')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = sklearn.metrics.pairwise.euclidean_distances(mutations_df_noNA)\n",
    "\n",
    "sns.heatmap(E, robust=True, square=True, yticklabels=False, xticklabels=False, cbar=True)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(np.hstack(E), 20, facecolor='orange', alpha=0.5)\n",
    "plt.xlabel('Distribution of pairwise distances')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLASS CODE#### ~for below cell\n",
    "def scatterplot_2D(R, title, labels=None):\n",
    "    \"\"\" Helper function to plot data points in 2D\n",
    "        Requires (N, 2) numpy array shape\n",
    "    \"\"\"\n",
    "    assert(R.shape[1] == 2)\n",
    "    # class labels are turned into colors\n",
    "    if labels is None:\n",
    "        c = 'black'\n",
    "    else:\n",
    "        color_scale = np.linspace(0, 1, len(set(labels)))\n",
    "        c = [plt.cm.Set1(color_scale[i]) for i in labels]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.patch.set_facecolor('white')\n",
    "    ax.scatter(R[...,0], R[...,1], color=c)\n",
    "    ax.axis('square')\n",
    "    ax.set_xlabel('R1')\n",
    "    ax.set_ylabel('R2')\n",
    "    fig.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-linear transformations:\n",
    "from sklearn.manifold import MDS, Isomap, TSNE\n",
    "\n",
    "R_MDS = MDS(n_components=2).fit_transform(mutations_df_noNA)\n",
    "scatterplot_2D(R_MDS, 'MDS') \n",
    "\n",
    "\n",
    "E = sklearn.metrics.pairwise_distances(R_MDS, metric='braycurtis', n_jobs=-1)\n",
    "# sns.heatmap(D, robust=True, square=True, yticklabels=True, xticklabels=True, cbar=True)\n",
    "# plt.show()\n",
    "\n",
    "plt.hist(np.hstack(E), 20, facecolor='orange', alpha=0.75)\n",
    "plt.xlabel('Pairwise distances')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_TSNE = TSNE(n_components=2, perplexity=20).fit_transform(mutations_df_noNA)\n",
    "scatterplot_2D(R_TSNE, 'TSNE') #once I define y in np array: scatterplot_2D(R_TSNE, 'TSNE', y), y for colour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_ISO = Isomap(n_components=2).fit_transform(mutations_df_noNA)\n",
    "scatterplot_2D(R_ISO, 'Isomap') #scatterplot_2D(R_ISO, 'Isomap', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA = sklearn.decomposition.PCA(n_components=2).fit_transform(mutations_df_noNA)\n",
    "scatterplot_3D(PCA, 'PCA') #scatterplot_2D(R_ISO, 'Isomap', y)\n",
    "\n",
    "\n",
    "E = sklearn.metrics.pairwise_distances(PCA, metric='braycurtis', n_jobs=-1)\n",
    "# sns.heatmap(D, robust=True, square=True, yticklabels=True, xticklabels=True, cbar=True)\n",
    "# plt.show()\n",
    "\n",
    "plt.hist(np.hstack(E), 20, facecolor='orange', alpha=0.75)\n",
    "plt.xlabel('Pairwise distances')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Principal Component Analysis\n",
    "R_PCA = sklearn.decomposition.PCA(n_components=2).fit_transform(X)\n",
    "scatterplot_2D(R_PCA, 'PCA', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We choose TSNE as DR method, moving on to cluster testing now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###THIS CODE IS NOT MINE, FROM CLASS##### #this is necessary for below of using kmeans\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_kmeans(original_data, cluster_labels, model, k):\n",
    "    \"\"\"Plot the clusters and centroids for a model trained on the Iris dataset.\"\"\"\n",
    "    # Scatter plots of each species\n",
    "    for cluster_idx in range(k):\n",
    "        plt.scatter(\n",
    "            original_data.iloc[cluster_labels == cluster_idx, 0],\n",
    "            original_data.iloc[cluster_labels == cluster_idx, 1],\n",
    "            label=\"Cluster #{}\".format(cluster_idx),\n",
    "        )\n",
    "    plt.legend()\n",
    "    #plt.scatter(original_data[cluster_labels == 1, 0], original_data[cluster_labels == 1, 1], s = 100, c = 'blue', label = 'Species 1')\n",
    "\n",
    "    # Add centroids to plot\n",
    "    plt.scatter(\n",
    "        model.cluster_centers_[:, 0],\n",
    "        model.cluster_centers_[:, 1],\n",
    "        s=100,\n",
    "        c='red',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=100)\n",
    "df_1_tsne = tsne.fit_transform(mutations_df_noNA) #the DR\n",
    "print(df_1_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tsne\n",
    "model.fit(df_1_tsne)\n",
    "cluster_labels = model.predict(df_1_tsne)\n",
    "plot_kmeans(pd.DataFrame(df_1_tsne), cluster_labels, model,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "ktest = KMeans(n_clusters=6, random_state=50).fit_predict(df_1_tsne) #using DR here see\n",
    "plt.subplot()\n",
    "plt.scatter(df_1_tsne[:, 0], df_1_tsne[:, 1], cmap='Paired', c=ktest, alpha = 0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular GMM\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=6).fit(df_1_tsne)\n",
    "labels = gmm.predict(df_1_tsne)\n",
    "plt.scatter(df_1_tsne[:, 0], df_1_tsne[:, 1], c=labels, s=40, cmap='Paired', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of how many clusters to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#below is better code from scikit, produces a slightly different output, but still the same conclusion of max 6 clusters. \n",
    "#5 may be better owing to size of plot bars being more similar thatn with c = 6, and the one cluster bar being far over avg silohuette score = overfitting (?)\n",
    "#matches the silhoette line plot above, not elbow though \n",
    "##can replace df_1_tsne with df_1_mds to view that. The results do not look as good as tsne (only a few clusters reasonably above averge, until higher # [overfitting]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###code not mine, from https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html####\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "range_n_clusters = [4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(df_1_tsne) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(df_1_tsne)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(df_1_tsne, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(df_1_tsne, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(df_1_tsne[:, 0], df_1_tsne[:, 1], marker='.', s=30, lw=0, alpha=0.5,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## April 2019 updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram for the cluster way we choose\n",
    "import scipy.cluster.hierarchy as shc\n",
    "\n",
    "plt.figure(figsize=(10, 7))  \n",
    "plt.title(\"Customer Dendograms\")  \n",
    "dend = shc.dendrogram(shc.linkage(df_1_tsne, method='ward'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actually need those model stats now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow method ##class code\n",
    "\n",
    "def plot_elbow(dataset, max_clusters):\n",
    "    \"\"\"Plot elbow curve for k-means.\"\"\"\n",
    "    inertias = []\n",
    "    for i in range(1, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=i, random_state=768797)\n",
    "        kmeans.fit(dataset)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "\n",
    "    plt.plot(range(1, max_clusters + 1), inertias)\n",
    "    plt.title(\"Elbow Plot\")\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"SSD\")\n",
    "\n",
    "plot_elbow(df_1_tsne, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette is used for assessing the performance of an unlabeled dataset ##class code\n",
    "# this is what's shown above in the graphs\n",
    "\n",
    "from sklearn.metrics.cluster import silhouette_score\n",
    "\n",
    "def calc_silhouette(dataset, n):\n",
    "    \"\"\"Runs Kmeans clustering and returns average silhouette coefficient\"\"\"    \n",
    "    kmeans = KMeans(n_clusters=n).fit(dataset)\n",
    "    score = silhouette_score(dataset, kmeans.labels_)\n",
    "    return score\n",
    "\n",
    "scores = {n: calc_silhouette(df_1_tsne, n) for n in range(2, 10)}\n",
    "# pprint.pprint(scores)\n",
    "plt.plot(\n",
    "    list(scores.keys()),\n",
    "    list(scores.values())\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Average silhouette coefficient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC / ROC\n",
    "#ROC CURVE WITH AUC SCORES\n",
    "\n",
    "# Classification and ROC analysis\n",
    "\n",
    "# Run classifier with cross-validation and plot ROC curves\n",
    "cv = StratifiedKFold(n_splits=6)\n",
    "classifier = svm.SVC(kernel='linear', probability=True,\n",
    "                     random_state=random_state)\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "i = 0\n",
    "for train, test in cv.split(X, y):\n",
    "    probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    i += 1\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AIC BIC\n",
    "from sklearn.mixture import GaussianMixture\n",
    "n_components = np.arange(1, 10)\n",
    "models = [GaussianMixture(n, covariance_type='full', random_state=0).fit(X)\n",
    "          for n in n_components]\n",
    "\n",
    "plt.plot(n_components, [m.bic(X) for m in models], label='BIC')\n",
    "plt.plot(n_components, [m.aic(X) for m in models], label='AIC')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('n_components');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a classification task using 5 informative features\n",
    "X, y = sklearn.datasets.make_classification(n_samples=1000, n_features=30, n_informative=5,\n",
    "                           n_redundant=4, n_repeated=0, n_classes=4,\n",
    "                           n_clusters_per_class=1, random_state=0,\n",
    "                                           class_sep=0.2)\n",
    "\n",
    "# Create the RFE object and compute a cross-validated score.\n",
    "svc = sklearn.svm.SVC(kernel=\"linear\", class_weight=\"balanced\")\n",
    "rfecv = sklearn.feature_selection.RFECV(estimator=svc,\n",
    "                                        step=1,\n",
    "                                        cv=sklearn.model_selection.StratifiedKFold(2),\n",
    "                                        scoring='accuracy')\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(\"Number of features corresponding to max CV score : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (# of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DR on PCA b/c gabe wants it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(mutations_df_noNA)\n",
    "PCA(copy=True, n_components=2, whiten=False)\n",
    "X = pca.transform(mutations_df_noNA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we can visualize the (transformed) iris dataset:\n",
    "import pylab as pl\n",
    "pl.scatter(X[:, 0], X[:, 1], c=iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D tsne for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(*zip(*tsne))\n",
    "plt.show()\n",
    "\n",
    "#plt.scatter(*zip(*tsne[:,:2]), c=X[:,2])\n",
    "#plt.scatter(*zip(*tsne[:,:2]), c=X[:,2], cmap='RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_index = mutations_df_noNA.index # DO I WANT THIS AS TRINUCLS OR MUTS....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = KMeans(n_clusters=9, random_state=160).fit_predict(df_1_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot()\n",
    "plt.scatter(df_1_tsne[:, 0], df_1_tsne[:, 1], c=y_pred) #this is the full df, it was reduced to two columns\n",
    "#this is all just a one line version of what we decided above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#but if we wanted to look at another method one more time (GMM)\n",
    "gmm = GaussianMixture(n_components=6).fit(df_1_tsne)\n",
    "labels = gmm.predict(df_1_tsne)\n",
    "plt.scatter(df_1_tsne[:, 0], df_1_tsne[:, 1], c=labels, s=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation - this method is not fully completed, to get true cluster validation, I need to pull which datasets this subset coresponds to and see if clustering matches with the full dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the model with different data (same data split/changed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_1, test_2 = train_test_split(df_1_tsne, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_2 = KMeans(n_clusters=6, random_state=160).fit_predict(test_2)\n",
    "plt.scatter(test_2[:, 0], test_2[:, 1], c=y_pred_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_1 = KMeans(n_clusters=6, random_state=160).fit_predict(test_1)\n",
    "plt.scatter(test_1[:, 0], test_1[:, 1], c=y_pred_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different method from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_1_tsne, df_1_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=X_train[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_testing = model.fit(X_train, y_train)\n",
    "predictions = kmeans.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ya idk but do qualitative tests too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, X_train)\n",
    "D\n",
    "#accuracy_score(y_true, y_pred, normalize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reassigning clusters to viruses (assumes dimensionality reduction did not change the order of the viruses/index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#..does tsne reorder data.....?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(digits.target, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(digits.target, labels)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=digits.target_names,\n",
    "            yticklabels=digits.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DR order retained\n",
    "yp = list(y_pred)\n",
    "namestemp =  list(id_index)\n",
    "clusterstemp = list(y_pred)\n",
    "#clusterstemp\n",
    "\n",
    "clustname = zip(namestemp, clusterstemp)\n",
    "final_out = list(zip(namestemp, clusterstemp))\n",
    "df_f_o = pd.DataFrame(final_out)\n",
    "\n",
    "df_f_o.head()\n",
    "df_f_o.to_csv(\"./25Feb_noRevComp/feb19_virus_to_NINE_TEST_clusters _match.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
